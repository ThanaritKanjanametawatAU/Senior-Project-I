{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **T5-Sentinel Model**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Train, Validation, and Test Split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 The preprocessed dataset is read from the csv file. The extracted features are converted from string representation of lists into actual lists of numbers. The lists of numbers are then converted into numpy arrays. The labels are also converted into numpy arrays."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the data\n",
    "combined_df = pd.read_csv('dataset/Combined-Preprocessed-Dataset.csv')\n",
    "# Randomly pick 100 data points from each class\n",
    "combined_df = combined_df.groupby('label').apply(lambda x: x.sample(n=100, random_state=1)).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:22:39.640312400Z",
     "start_time": "2023-09-12T12:22:32.231312500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 The data is split into train, validation, and test sets. The train set is 67.5% of the data, the validation set is 12.5% of the data, and the test set is 20% of the data. Validation set is used to tune the hyperparameters of the model. The test set is used to evaluate the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess data\n",
    "\n",
    "# Modify labels for T5 classification\n",
    "combined_df['t5_label'] = combined_df['label'].apply(lambda x: \"positive </s>\" if x == 1 else \"negative </s>\")\n",
    "\n",
    "# Initialize T5 tokenizer and model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "\n",
    "# Encode texts and labels\n",
    "texts = combined_df['text'].tolist()\n",
    "labels = combined_df['t5_label'].tolist()\n",
    "\n",
    "# Tokenize\n",
    "input_encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\").to(device)\n",
    "input_ids = input_encodings['input_ids']\n",
    "attention_masks = input_encodings['attention_mask']\n",
    "\n",
    "# Data Split\n",
    "train_inputs, test_inputs, train_masks, test_masks = train_test_split(input_ids, attention_masks, test_size=0.2)\n",
    "train_labels, test_labels = train_test_split(combined_df['t5_label'].values, test_size=0.2)\n",
    "train_inputs, val_inputs, train_masks, val_masks = train_test_split(train_inputs, train_masks, test_size=0.125)\n",
    "train_labels, val_labels = train_test_split(train_labels, test_size=0.125)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:22:43.617315Z",
     "start_time": "2023-09-12T12:22:39.644318900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "count               200\nunique                2\ntop       negative </s>\nfreq                100\nName: t5_label, dtype: object"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['t5_label'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:22:43.649314100Z",
     "start_time": "2023-09-12T12:22:43.649314100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u6410322/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 39\u001B[0m\n\u001B[1;32m     37\u001B[0m input_batch \u001B[38;5;241m=\u001B[39m train_inputs[i:i\u001B[38;5;241m+\u001B[39mbatch_size]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     38\u001B[0m mask_batch \u001B[38;5;241m=\u001B[39m train_masks[i:i\u001B[38;5;241m+\u001B[39mbatch_size]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 39\u001B[0m label_batch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(train_labels[i:i\u001B[38;5;241m+\u001B[39mbatch_size])\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     42\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     43\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(input_ids\u001B[38;5;241m=\u001B[39minput_batch, attention_mask\u001B[38;5;241m=\u001B[39mmask_batch, labels\u001B[38;5;241m=\u001B[39mlabel_batch)\n",
      "\u001B[0;31mTypeError\u001B[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 1 \n",
    "learning_rate = 5e-4\n",
    "weight_decay = 1e-3\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_inputs) * epochs)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "train_precisions = []\n",
    "val_precisions = []\n",
    "train_f1s = []\n",
    "val_f1s = []\n",
    "\n",
    "# Fine-tune T5 model\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_preds, train_true = [], []\n",
    "\n",
    "    for i in range(0, len(train_inputs), batch_size):\n",
    "        input_batch = train_inputs[i:i+batch_size].to(device)\n",
    "        mask_batch = train_masks[i:i+batch_size].to(device)\n",
    "        label_batch = torch.tensor(train_labels[i:i+batch_size]).to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_batch, attention_mask=mask_batch, labels=label_batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits.argmax(dim=-1)[:, 0].detach().cpu().numpy()  # Take the first token's predicted ID\n",
    "        labels = label_batch[:, 0].cpu().numpy()  # Take the first token's true ID\n",
    "        train_preds.extend(logits)\n",
    "        train_true.extend(labels)\n",
    "\n",
    "    # Calculate metrics for training\n",
    "    train_accuracy = accuracy_score(train_true, train_preds)\n",
    "    train_precision = precision_score(train_true, train_preds, average='weighted')\n",
    "    train_f1 = f1_score(train_true, train_preds, average='weighted')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(val_inputs), batch_size):\n",
    "            input_batch = val_inputs[i:i+batch_size].to(device)\n",
    "            label_batch = val_labels[i:i+batch_size].to(device)\n",
    "            outputs = model(input_ids=input_batch, labels=label_batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "            # For accuracy, precision, f1\n",
    "            logits = outputs.logits.argmax(dim=-1)[:, 0].detach().cpu().numpy()  # Take the first token's predicted ID\n",
    "            labels = label_batch[:, 0].cpu().numpy()  # Take the first token's true ID\n",
    "            val_preds.extend(logits)\n",
    "            val_true.extend(labels)\n",
    "\n",
    "    # Calculate metrics for validation\n",
    "    val_accuracy = accuracy_score(val_true, val_preds)\n",
    "    val_precision = precision_score(val_true, val_preds, average='weighted')\n",
    "    val_f1 = f1_score(val_true, val_preds, average='weighted')\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(running_loss/len(train_inputs))\n",
    "    val_losses.append(val_loss/len(val_inputs))\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    train_precisions.append(train_precision)\n",
    "    val_precisions.append(val_precision)\n",
    "    train_f1s.append(train_f1)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:22:44.108317200Z",
     "start_time": "2023-09-12T12:22:43.649314100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting metrics\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plotting Loss\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Accuracy\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Precision\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(train_precisions, label='Train Precision')\n",
    "plt.plot(val_precisions, label='Val Precision')\n",
    "plt.title('Precision')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting F1 Score\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(train_f1s, label='Train F1')\n",
    "plt.plot(val_f1s, label='Val F1')\n",
    "plt.title('F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:22:44.107312200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns\n",
    "# Evaluate the model on the test set\n",
    "test_preds, test_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_inputs), batch_size):\n",
    "        input_batch = test_inputs[i:i+batch_size].to(device)\n",
    "        label_batch = test_labels[i:i+batch_size].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_batch, labels=label_batch)\n",
    "        # For accuracy, precision, f1\n",
    "        logits = outputs.logits.argmax(dim=-1)[:, 0].detach().cpu().numpy()  # Take the first token's predicted ID\n",
    "        labels = label_batch[:, 0].cpu().numpy()  # Take the first token's true ID\n",
    "        test_preds.extend(logits)\n",
    "        test_true.extend(labels)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_accuracy = accuracy_score(test_true, test_preds)\n",
    "test_precision = precision_score(test_true, test_preds, average='weighted')\n",
    "test_recall = recall_score(test_true, test_preds, average='weighted')\n",
    "test_f1 = f1_score(test_true, test_preds, average='weighted')\n",
    "\n",
    "# Handle both binary and multi-class cases for FPR and FNR\n",
    "confusion = confusion_matrix(test_true, test_preds)\n",
    "if confusion.shape == (2, 2):  # Binary classification\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "else:  # Multi-class classification\n",
    "    fp = confusion.sum(axis=0) - np.diag(confusion)\n",
    "    fn = confusion.sum(axis=1) - np.diag(confusion)\n",
    "    tp = np.diag(confusion)\n",
    "    tn = confusion.sum() - (fp + fn + tp)\n",
    "    fpr = fp.sum() / (fp.sum() + tn.sum())\n",
    "    fnr = fn.sum() / (fn.sum() + tp.sum())\n",
    "\n",
    "# Display metrics\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:22:44.108317200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ROC Curve & AUC\n",
    "from sklearn.preprocessing import label_binarize\n",
    "test_true_bin = label_binarize(test_true, classes=[0, 1])\n",
    "test_preds_bin = label_binarize(test_preds, classes=[0, 1])\n",
    "fpr_roc, tpr_roc, _ = roc_curve(test_true_bin, test_preds_bin)\n",
    "roc_auc = auc(fpr_roc, tpr_roc)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_roc, tpr_roc, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:22:44.114318900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DET Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, fnr, color='darkorange', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('False Negative Rate')\n",
    "plt.title('Detection Error Tradeoff (DET) Curve')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:22:44.118317800Z",
     "start_time": "2023-09-12T12:22:44.114318900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 't5_sentinel_model.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:22:44.114318900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:22:44.114318900Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
