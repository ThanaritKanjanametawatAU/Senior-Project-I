{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-27T09:00:54.125891600Z",
     "start_time": "2023-08-27T09:00:52.641121600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                      uid                                               text  \\\n0   [urlsf_subset00]-[83]  The National Weather Service's Mike McFarland ...   \n1   [urlsf_subset00]-[89]  The President of the United States was seen on...   \n2  [urlsf_subset00]-[390]  Enner Valencia scored two goals in Ecuador's 2...   \n3  [urlsf_subset00]-[457]  Beginning with the introduction, the author sh...   \n4  [urlsf_subset00]-[458]  Mexico has implemented its newest data retenti...   \n\n   label  \n0      1  \n1      1  \n2      1  \n3      1  \n4      1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uid</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[urlsf_subset00]-[83]</td>\n      <td>The National Weather Service's Mike McFarland ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[urlsf_subset00]-[89]</td>\n      <td>The President of the United States was seen on...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[urlsf_subset00]-[390]</td>\n      <td>Enner Valencia scored two goals in Ecuador's 2...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[urlsf_subset00]-[457]</td>\n      <td>Beginning with the introduction, the author sh...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[urlsf_subset00]-[458]</td>\n      <td>Mexico has implemented its newest data retenti...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ChatGPT = []\n",
    "OpenWeb = []\n",
    "\n",
    "# Read all the jsonl files in the path\n",
    "for i in range(8):\n",
    "    a = pd.read_json(f\"dataset/opengpttext-clean/chatgpt/urlsf_subset0{i}.jsonl\", lines=True)\n",
    "    b = pd.read_json(f\"dataset/opengpttext-clean/openweb/urlsf_subset0{i}.jsonl\", lines=True)\n",
    "    ChatGPT.append(a)\n",
    "    OpenWeb.append(b)\n",
    "\n",
    "# Combine dataframes of the same type\n",
    "chatgpt_df = pd.concat(ChatGPT, ignore_index=True)\n",
    "openweb_df = pd.concat(OpenWeb, ignore_index=True)\n",
    "\n",
    "# Assign labels\n",
    "chatgpt_df['label'] = 1\n",
    "openweb_df['label'] = 0\n",
    "\n",
    "# Combine both labeled dataframes\n",
    "combined_df = pd.concat([chatgpt_df, openweb_df], ignore_index=True)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import RobertaTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "\n",
    "\n",
    "# Tokenize data\n",
    "texts = combined_df['text'].tolist()\n",
    "labels = combined_df['label'].tolist()\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "encoding = tokenizer(texts, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_input_ids, test_input_ids, train_labels, test_labels = train_test_split(encoding['input_ids'], labels, test_size=0.2, random_state=42)\n",
    "train_attention_mask, test_attention_mask = train_test_split(encoding['attention_mask'], test_size=0.2, random_state=42)\n",
    "\n",
    "train_input_ids, val_input_ids, train_labels, val_labels = train_test_split(train_input_ids, train_labels, test_size=0.125, random_state=42)\n",
    "train_attention_mask, val_attention_mask = train_test_split(train_attention_mask, test_size=0.125, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, torch.tensor(train_labels))\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, torch.tensor(val_labels))\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, torch.tensor(test_labels))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T09:47:47.269165800Z",
     "start_time": "2023-08-27T09:44:43.956638700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([    0,   243,    16,  3159,    13, 23811,     7,  3271,  8109,    49,\n          6160,    50,   904,    49,  5086,    15,    49,  4476,     6,    53,\n          3533, 32194,   222,    95,    14,    71,  9585,   154,     5,  1573,\n           166,   808,   397,  1954,     4,  1190,  6760,  3979,  1032,    23,\n          5062,    15,   274,   791,  3721,   204,     4,    91,  2641,    14,\n            37,   905,     5,  1032,   213,    15,   350,   251,     6,  3735,\n          6760,  3979,     7,   185,   350,   171, 10495, 19250,     4,   152,\n          7988,    16,    10, 17846,   517,    25, 23811,    32,   747,  5888,\n            53,  7154,   547,  8943,     4, 50118, 14229,     5,  1032,     6,\n           166,   808,   397,  1882,  6760,  3979,    19,    10,  2934, 13789,\n          2506,     8,  1143,     7, 10064,   123,    15,     5,  7821,    13,\n            59,   799,  2397,     4, 32194,   115,    33,  2294,     5,  1032,\n           656,     7,  2097,  6760,  3979,    31,   602,   143,   617,  1880,\n             4,    91,  2641,    14,    37,    21,    22, 33234,    15,     5,\n          7691,   113,     8,   905,     5,  1032,   213,    15,    10,   367,\n         19594,   350,   251,     4,   152, 20516,     5,  4496,    14, 10290,\n            11,   235,    71,     5,  1032,     4, 50118, 33118,   394, 11014,\n           735,    67,  5888,     5, 16133,  1580,     6,  1996,   141, 32194,\n           115,    22,  1610,  2934,   101,    42,     6,   546,   159,    23,\n            10,  2173,   562, 22355,   101,    14,     8,    45,   206,     5,\n          1032,   782,     7,    28,  2294,  1917, 32194,  5055,    14,     5,\n          3633,    37,   829,    21,  2105,     8,    14,    37,  1220,  6760,\n          3979,     7,   535,  2190,  1181,    87,    37,   197,    33,     4,\n         50118, 15243,    39,  5021,     6, 32194,    16,   684,    13, 13455,\n          2455,     5,  2503,  8400,  1954,     4, 12381, 15025,    22,  3609,\n         37803,   113,   248,  4324,  1032,     7,   535,   454,     5,   507,\n         21305,  1135,  7353, 15706,    30,   258,  5464,     4,    91,  2775,\n            14,    37,  5741,     7,   904,   349,  7251,   358,   945,     7,\n           535,     6,   941,   576,     5,   801,  8158,   963,    11,    10,\n          9953,   227,    80,   299,   158, 13372,     4,   635,     6,    11,\n         30690,     6,    37,  2641,    14,    37,  9010,   350,   251,     7,\n           912,     5,  1032,   227,   166,   808,   397,     8,  6760,  3979,\n             4, 50118, 14693,     7, 32194,     6,  9563,    16,    10,  1233,\n           233,     9,     5,  2414,     6,     8,    37,    16,  2882,     7,\n           185,  2640,    13,    39,  2163,     4,    91,  1991,     7,  1149,\n            39,   177,    62,     8,  1306,    14,    37,    16,    15,   477,\n            13,     5,   220,  5464,     4,  2223, 11080,    16,    10,  6744,\n          2414,     6, 32194,   473,    45,   236,     7,   192,  5464,   185,\n         10495,  8653,     6,     8,    37,    16,  2882,     7,  1532,    31,\n            39,  6160,     7,  1477,    25,    10,  9585,     4,     2,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n             1,     1]),\n tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n tensor(1))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-27T09:49:47.121217200Z",
     "start_time": "2023-08-27T09:49:47.115255Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import RobertaModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class RobertaSentinel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaSentinel, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.fc1 = nn.Linear(self.roberta.config.hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids, attention_mask)\n",
    "        cls_token = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.gelu(self.fc1(cls_token))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Freeze the RoBERTa layers\n",
    "model = RobertaSentinel()\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len(train_dataset)//512, eta_min=0)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Training loop with gradient accumulation\n",
    "epochs = 15\n",
    "accumulation_steps = 4\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (input_ids, attention_mask, labels) in enumerate(DataLoader(train_dataset, batch_size=512, shuffle=True)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids.to(device), attention_mask.to(device))\n",
    "        loss = loss_fn(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        if (i+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_dataset)}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in DataLoader(test_dataset, batch_size=512):\n",
    "        outputs = model(input_ids.to(device), attention_mask.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to(device)).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {correct/total*100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
