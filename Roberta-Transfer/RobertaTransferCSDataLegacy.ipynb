{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:29:05.809023200Z",
     "start_time": "2023-09-12T12:28:32.448846600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     Paper ID                                               text  label  \\\n0  1905.02253  This paper presents Bee$^+$, a novel insect-sc...      1   \n1  2010.14244  This paper proposes a novel approach for enabl...      1   \n2  2011.00330  This paper proposes a novel approach to overco...      1   \n3  2011.02250  This paper provides a comprehensive review of ...      1   \n4  2102.04525  The paper presents a novel loss function calle...      1   \n\n                                  extracted_features  \n0  [-5.34785949e-02  1.15970001e-01 -2.49453727e-...  \n1  [-1.11627234e-02  1.36427671e-01 -2.40767486e-...  \n2  [-1.38635410e-03  1.07046261e-01 -3.00604366e-...  \n3  [-4.02924139e-03  1.25577912e-01 -2.42507644e-...  \n4  [-1.11521436e-02  1.01311266e-01 -3.34746987e-...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Paper ID</th>\n      <th>text</th>\n      <th>label</th>\n      <th>extracted_features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1905.02253</td>\n      <td>This paper presents Bee$^+$, a novel insect-sc...</td>\n      <td>1</td>\n      <td>[-5.34785949e-02  1.15970001e-01 -2.49453727e-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010.14244</td>\n      <td>This paper proposes a novel approach for enabl...</td>\n      <td>1</td>\n      <td>[-1.11627234e-02  1.36427671e-01 -2.40767486e-...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011.00330</td>\n      <td>This paper proposes a novel approach to overco...</td>\n      <td>1</td>\n      <td>[-1.38635410e-03  1.07046261e-01 -3.00604366e-...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2011.02250</td>\n      <td>This paper provides a comprehensive review of ...</td>\n      <td>1</td>\n      <td>[-4.02924139e-03  1.25577912e-01 -2.42507644e-...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2102.04525</td>\n      <td>The paper presents a novel loss function calle...</td>\n      <td>1</td>\n      <td>[-1.11521436e-02  1.01311266e-01 -3.34746987e-...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the data\n",
    "combined_df = pd.read_csv('dataset/CS-Abstract-Combined-Preprocessed-Dataset.csv')\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(combined_df, test_size=0.3, stratify=combined_df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'])\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_df['text'].tolist(), truncation=True, padding=True, max_length=512)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:32:47.504240700Z",
     "start_time": "2023-09-12T12:29:05.806033300Z"
    }
   },
   "id": "d043e2c5834cddb5"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m TensorDataset(train_encodings\u001B[38;5;241m.\u001B[39minput_ids, train_encodings\u001B[38;5;241m.\u001B[39mattention_mask, torch\u001B[38;5;241m.\u001B[39mtensor(train_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()))\n\u001B[1;32m      2\u001B[0m val_dataset \u001B[38;5;241m=\u001B[39m TensorDataset(val_encodings\u001B[38;5;241m.\u001B[39minput_ids, val_encodings\u001B[38;5;241m.\u001B[39mattention_mask, torch\u001B[38;5;241m.\u001B[39mtensor(val_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()))\n\u001B[1;32m      3\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:192\u001B[0m, in \u001B[0;36mTensorDataset.__init__\u001B[0;34m(self, *tensors)\u001B[0m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mtensors: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 192\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(tensors[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m==\u001B[39m tensor\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m tensor \u001B[38;5;129;01min\u001B[39;00m tensors), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize mismatch between tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors \u001B[38;5;241m=\u001B[39m tensors\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:192\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mtensors: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 192\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(tensors[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m==\u001B[39m tensor\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m tensor \u001B[38;5;129;01min\u001B[39;00m tensors), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize mismatch between tensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtensors \u001B[38;5;241m=\u001B[39m tensors\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(train_encodings.input_ids, train_encodings.attention_mask, torch.tensor(train_df['label'].tolist()))\n",
    "val_dataset = TensorDataset(val_encodings.input_ids, val_encodings.attention_mask, torch.tensor(val_df['label'].tolist()))\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:32:47.505242700Z",
     "start_time": "2023-09-12T12:32:47.504240700Z"
    }
   },
   "id": "c3370adf1f19f19b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 4: Model Initialization\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:32:47.692241400Z",
     "start_time": "2023-09-12T12:32:47.692241400Z"
    }
   },
   "id": "df565d9d773d1544"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 5: Training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "# Initialize lists to store metrics\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "train_accuracy_values = []\n",
    "val_accuracy_values = []\n",
    "train_precision_values = []\n",
    "val_precision_values = []\n",
    "train_f1_values = []\n",
    "val_f1_values = []\n",
    "\n",
    "# Training Loop with Evaluation\n",
    "for epoch in range(3):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_predictions, train_true_labels = [], []\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        train_predictions.extend(torch.argmax(logits, axis=1).tolist())\n",
    "        train_true_labels.extend(labels.tolist())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_loss_values.append(avg_train_loss)\n",
    "\n",
    "    train_accuracy = accuracy_score(train_true_labels, train_predictions)\n",
    "    train_precision = precision_score(train_true_labels, train_predictions)\n",
    "    train_f1 = f1_score(train_true_labels, train_predictions)\n",
    "\n",
    "    train_accuracy_values.append(train_accuracy)\n",
    "    train_precision_values.append(train_precision)\n",
    "    train_f1_values.append(train_f1)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_predictions, val_true_labels = [], []\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        total_val_loss += outputs.loss.item()\n",
    "        logits = outputs.logits\n",
    "        val_predictions.extend(torch.argmax(logits, axis=1).tolist())\n",
    "        val_true_labels.extend(labels.tolist())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_loss_values.append(avg_val_loss)\n",
    "\n",
    "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "    val_precision = precision_score(val_true_labels, val_predictions)\n",
    "    val_f1 = f1_score(val_true_labels, val_predictions)\n",
    "\n",
    "    val_accuracy_values.append(val_accuracy)\n",
    "    val_precision_values.append(val_precision)\n",
    "    val_f1_values.append(val_f1)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Precision: {train_precision:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Loss: {val_losses[-1]:.4f}, Val Accuracy: {val_accuracy:.4f}, Val Precision: {val_precision:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    print('-----------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:32:47.692241400Z",
     "start_time": "2023-09-12T12:32:47.692241400Z"
    }
   },
   "id": "4a71f29675706c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting metrics\n",
    "epochs = range(1, 4)\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs, train_loss_values, label='Training Loss')\n",
    "plt.plot(epochs, val_loss_values, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs, train_accuracy_values, label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy_values, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(epochs, train_precision_values, label='Training Precision')\n",
    "plt.plot(epochs, val_precision_values, label='Validation Precision')\n",
    "plt.title('Training and Validation Precision')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(epochs, train_f1_values, label='Training F1')\n",
    "plt.plot(epochs, val_f1_values, label='Validation F1')\n",
    "plt.title('Training and Validation F1 Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-12T12:32:47.692241400Z",
     "start_time": "2023-09-12T12:32:47.692241400Z"
    }
   },
   "id": "678f9922f6463676"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 6: Evaluation\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "for batch in val_loader:\n",
    "    input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(torch.argmax(logits, axis=1).tolist())\n",
    "    true_labels.extend(labels.tolist())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "auroc = roc_auc_score(true_labels, predictions)\n",
    "fpr, tpr, _ = roc_curve(true_labels, predictions)\n",
    "eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "# Plotting ROC and DET Curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {auroc})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}, AUROC: {auroc}, EER: {eer}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-12T12:32:47.737242200Z"
    }
   },
   "id": "f244d28aa9ee63d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
